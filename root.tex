%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8

\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  % Comment this line out if you need a4paper

\IEEEoverridecommandlockouts                              % This command is only needed if 
                                                          % you want to use the \thanks command

\overrideIEEEmargins                                      % Needed to meet printer requirements.

\usepackage[bookmarks=true]{hyperref}
\usepackage{amsmath,algorithm}
\usepackage{amsfonts}
\usepackage[noend]{algpseudocode}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{makecell}
\usepackage{float}
\usepackage{tikz}
\usepackage{pgfplots} 
\usepackage{caption}
\usepackage{subcaption}
\usetikzlibrary{patterns}
\usetikzlibrary{positioning,shapes.misc}
\tikzset{
    myboxrectangle/.style={rectangle,draw=black,align=center, minimum width=2cm, minimum height=0.8cm},
}
\DeclareMathOperator*{\argmax}{argmax} % thin space, limits underneath in displays
\DeclareMathOperator*{\argmin}{argmin} % thin space, limits underneath in displays

\title{\LARGE \bf
Path-Tree planning using Rapidly-exploring Random Graphs for partially observable environments
}

\author{Camille Phiquepal$^{1}$, Andreas Orthey$^{2}$, Nicolas Viennot$^{3}$ and Marc Toussaint$^{4}$
\thanks{$^{1}$Machine Learning \& Robotic Lab, University of Stuttgart, Germany
        {\tt\small camille.phiquepal@ipvs.uni-stuttgart.de}}
\thanks{$^{2}$Real time robotics, Berlin, Germany
        {\tt\small aorthey@rtr.ai}}
\thanks{$^{3}${\tt\small nicolas.viennot@cs.columbia.edu}}
\thanks{$^{4}$Learning and Intelligent Systems Lab, TU Berlin, Germany
        {\tt\small toussaint@tu-berlin.de}}
}

\begin{document}



\maketitle
\thispagestyle{empty}
\pagestyle{empty}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
This paper presents a new approach to path-planning for problems where essential and discrete aspects of the environment are partially observable.
To achieve its task, the robot must be able to observe and infer knowledge about its environment.
We introduce the ROPT (Random Optimal Path-Tree) algorithm which consists in planning a \textit{path-tree} in belief-space. Path-trees are tree-like motions with branching points where the robot receives an observation leading to a belief state update. The robot takes different branches depending on the observation received.
The algorithm has three main steps. First, a rapidly-exploring random graph (RRG) is grown. The RRG is grown similarly to RRT methods but has a graph structure. Second, the RRG is expanded to belief space by querying the observation model. In a third step, dynamic programing is performed on the belief space graph to extract a path-tree. The resulting path-tree combines exploration with exploitation i.e.\ it balances the need for gaining knowledge about the environment with the need for reaching the goal. We demonstrate the capabilities of the approach on navigation and mobile manipulation tasks.
% domain agnostic way
% More optimized towards likely states than unlikely ones
% modular algorithm
% exhibits exploration
% The planning procedure uses an observation model, inference
% generalizes rrt
% belief state
% inference
% react to observation
% in simulation on 2 kinds of examples (mobile robot, robot arm mounted on a robot arm)
\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{INTRODUCTION}
In the field of path-planning, the environment is often assumed to be fully known. Partial observability, when considered, is often modeled as a limited and continuous variation around a central hypothesis  (e.g. addition of potential fields around obstacles, description of the robot localization uncertainty with gaussians). Planning itself is achieved using one single world representation.

In contrast, our approach aims to plan motions for problems where the robot knowledge about its environment is \textit{multi-modal}: The robot has a countable number of hypotheses about its environment. In practice, this can model obstructions in the environment the (see section\ref{experiment:navigation}), or the uncertainty about an object location (see section \ref{experiment:mobile_manipulation}), or any other uncertainty well represented as a discrete set of hypotheses.

This paper extends previous research work \cite{tamp-1}\cite{control-tree-1} which develop optimization based-methods for trajectory-tree planning in the respective subfields of Task and Motion Planning (TAMP) and Model Predictive Control (MPC). It extends the concept of tree-like motions to sampling-based path planning, and leverages the strengths of sampling-based approaches (especially the strong guarantees regarding collision avoidance and the asymptotic completeness) to tackle problems where the high number of occluding obstacles would be challenging for pure optimization-based methods (e.g. due to a high number of local minima). %environement complexity, environement structure, instead of occluding obstacles?
In addition, unlike \cite{tamp-1}, in which the observation actions are planned at the task level, the presented approach incorporates the observation model and the belief-state inference on the motion planning level directly, leading to unified algorithm.
% better sell the contributions?
%The main contribution of this work is

%The method is applicable to XXX and it is demonstrated on YYY

%This paper focuses on sampling based path planning. The main motivation for a sampling based methods in this context is the correlation between the observability problem and the collision avoidance problem, where sampling-based algorithm perform well: Partial observability is often due to obstacles occluding the robots field of view. In a similar way, obstacles prevent the robot to move freely. The ambition of the algorithm is to be a planning step solving both the problem of "where to move" and "where to observe" in complex environements.

%discrete partial observability vs continuous one -> DONE
%genesis
%motivation: strong link between observability pb and collision avoidance
%assumptions: multimodal - enumerable set of possible worlds, possibility to do inference
%integration in TAMP pipelines
%contribution / originality of the method: 
\section{RELATED WORK}
%A rich body of work has focused..

Path planning under partial observability and multi-modality of the environment state is related to the broader topic of path-planning under uncertainty for which many adaptations of classical sampling-based path planning algorithms like RRT \cite{lavalle1998rapidly} and PRM \cite{kavraki1996probabilistic} have been developed.

% use case: localization
One group such algorithms aims to tackle the localization uncertainty by planning paths which not only reach a goal but also minimize the localization uncertainty \cite{prentice2010belief} \cite{bry2011rapidly}. Like the presented approach, those algorithms plan in belief space on a graph grown in a sampling-based fashion. An observation model is used to infer the belief state dynamic. However, unlike the presented approach, those research model partial-observability over continuous variables (the localization), and assume gaussian belief-states. In addition we plan reactive path-trees reacting to the different possible observations.

% use case: mapping
Another closely related line of work plans paths maximizing the information gathered along the path \cite{hollinger2014sampling} \cite{levine2010information} \cite{dang2020graph}. Like the previously mentioned work, a graph or tree is grown in a sampling-based fashion. However, instead of planning in belief space, those approaches determine the optimal traversal w.r.t. an information objective, e.g. volumetric gain for the use case of mapping in \cite{dang2020graph}.


% One line of work can be qualified as "observation-aware" algorithms where an observation model is used to
%The modeled uncertainty is typically the localization (either of the robot or of the obstacles). 
% use case: monitoring
Another direction of research uses sampling based methods for planning problems where the goal is expressed as a temporal logic specification \cite{karaman2009sampling} \cite{leahy2019control} \cite{vasile2020reactive}. Like the method of this paper, the output solution of the planning problem is more general than a sequence of states. In \cite{karaman2009sampling} \cite{vasile2020reactive}, the solutions are cyclic infinite paths. The closest to our proposed approach is \cite{leahy2019control}. It synthesizes control policies in belief space that react to observations. The goal is specified using GDTL (Gaussian Distribution Temporal Logic). Like the other belief space planning approaches previously mentioned (\cite{prentice2010belief} \cite{bry2011rapidly}), the considered uncertainty is over the robot localization, and belief states are modeled as gaussians. Kalman filters are used for the belief states update. In contrast, the presented approach focuses on the environment uncertainty, it doesn't assume gaussian beliefs but describes the environment uncertainty as a finite and countable set of hypotheses.
% extension of the GDTL
% transition system in belief space
% localization 
% feedback controller
% 2 stages
% synthetize control policy
% goal condition specifies constraints about the localization uncertainty
% uncertainty seen as noise
% FIRM Feedback Information Roadmap
% policy that satisfies the goal with a high enough satisfaction probability
% with the crucial difference that the uncertainty is continuous and over the robot localization and assumed to be gaussian. On the other hand our 
% belief update with kalman filter

%It  but can take the form of cycles.
%%XX is the closest related work

% connect to use case.. monitoring


%In particular xy show similaities with our approach in that the combine a first step of graph growing in belief space and second step of path extraction.

%Like our approach 

%path planning in the presence of goal uncertainty; 
%Belief space path planning
%Non-linear policies RRG
%View planning
%POMDP Navigation

\section{PROBLEM FORMULATION}

\begin{figure}[!htb]
 \center{\includegraphics[width=0.25\textwidth]{drawings/problem.pdf}}
 \caption{Example of planning problem: The robot must reach the goal area. The state of a door (open vs. closed) is only observable in its vicinity. }
 \label{fig:problem}
\end{figure}

\subsection{State representation}
We optimize policies in a context of mixed-observability. The robot state is fully observable, but discrete and essential parts of the environment are only partially observed. In the example of the Figure \ref{fig:problem} this models the fact that each door can be open or closed.
 
To capture this mixed-observability structure, we adopt a compound state representation where a state is composed of 2 parts:
\begin{itemize}
\item $x \in \mathbb{R}^n$ is a continuous state and corresponds to the usual notion of state in the path planning literature.
\item $s \in \mathcal{H}$ is a discrete state from a finite set of world hypotheses $\mathcal{H}$.
\end{itemize}

In the problem introduced on Figure \ref{fig:problem} the variable $s$ can take four possible values corresponding to the possible states of the environment as illustrated on Figure \ref{fig:multiple_worlds}.

\begin{figure}[!htb]
 \center{\includegraphics[width=0.45\textwidth]{drawings/multiple_worlds.pdf}}
 \caption{Partially observable discrete state: With 2 partially observable doors, there are 4 possible states of the environment.}
 \label{fig:multiple_worlds}
\end{figure}

The robot is not oblivious about the likelihood of each state hypothesis. On the problem introduced on Figure \ref{fig:problem}, when the robot is in the vicinity of a door (symbolized with the circles), it receives an observation indicating whether the door is open or not.

Planning is performed is belief space, a belief state $b$ is a probability distribution over the different possible state hypotheses.

\subsection{Path-tree}
The algorithm consists in planning path-trees. As schematically shown on Figure \ref{fig:path_tree}, the path-tree starts from a single root node and finishes with leaf nodes satisfying the goal condition. The path-tree branches where the robot receives an observation leading to a belief state update. The different branches of the path-tree are the different planned contingencies. To be complete, a path-tree must have a path going to the goal region for all $s \in \mathcal{H}$.

In addition, we assume the observation model to be binary, meaning that $p(s|o) \in \{0.0, 1.0\}, \forall s \in \mathcal{H}, \forall o \in O$, where $o$ is an observation from the observation space $O$. This property is important. It guarantees that the number of belief-states is finite and can be enumerated which is needed for
the graph expansion to belief state presented in \ref{section:graph_expansion_belief_space}. For the example of Figure \ref{fig:problem}, it means that observations indicate whether the door is open or not without uncertainty. In addition, it typically implies that the number of branchings $N_o$ stays small compared to the total number of states $N$ on the path-tree. In other words $N_o \ll N$. It can be understood easily on the presented example: once an observation has been received for a door, the agent knows with certainty if the door is open or not, such that the next observations of the same door don't lead to an update of the belief-state.

\begin{figure}[!htb]
 \center{\includegraphics[width=0.25\textwidth]{drawings/path_tree_example.pdf}}
 \caption{Example of path-tree: The path-tree branches where an observation leads to a belief state update. There are 2 planned contingencies for each branching corresponding to the 2 cases (door open or closed).}
 \label{fig:path_tree}
\end{figure}

\subsection{Optimization objective}
We note $\psi$ a path tree, and $(u, v)$ consecutive nodes on the tree $\psi$. The motion cost between two nodes $u$ and $v$ is given by a cost function $C(u, v)$. In addition, we note $p(u | \psi, b_0)$ the probability to reach a node $v$. This probability depends on the initial belief state $b_0$ and the observation model. 

In the presence of uncertainty we minimize the expectation of the motion costs, such that the partially-observable multi-modal planning problem is defined as follows.
\begin{subequations}
\label{eq:optimization_objective}
\begin{align}
\psi^* = &\argmin_{\psi} \sum_{(u, v) \in \psi} C(u, v) p(v | \psi, b_0), \label{eq:cost_min}\\
&\text{s.t.} \notag \\
&\forall s \in \mathcal{H}\ \exists\ l \in \mathcal{L}(\psi) \ |\ G(l), \label{eq:goal_constraint}\\
&\mathcal{V}(u, v), \ \forall (u, v) \in \psi \label{eq:validity}, \\
&b_v(s) = p(s | o) \times b_u(s), \forall s \in \mathcal{H}, \forall (u, o, v) \in \psi, \label{eq:belief_dynamic}
\end{align}
\end{subequations}
where the $\mathcal{L}(\psi)$ gives the leaf nodes of $\psi$, and $G(l)$ is the goal predicate, indicating whether a node fulfills the goal conditions. \ref{eq:goal_constraint} states that $\psi$ shall be complete i.e. there is a leaf node satisfying the goal conditions for each possible $s$.

The constraint \ref{eq:validity} expresses that $\psi$ shall be composed of feasible motions (e.g. collision free, feasible given the robot motion model), and $\mathcal{V}(u, v)$ is the predicate encoding the validity of the transition between $u$ and $v$.

Finally, the constraint \ref{eq:belief_dynamic} corresponds to the bayesian updates according to the observation model. The symbol $o$ is the observation received when transitioning from $u$ to $v$. The belief state of the node $v$ is noted $b_v$, and $p(s|o)$ is the observation model.

It is noteworthy, that the problem formulation doesn't contain any term explicitly inciting the robot to explore its environment. The balance exploration / exploitation emerges naturally as a result of the minimization of the expected motion costs.
% difference compared to general POMDP

\section{Random Optimal Path-Tree algorithm (ROPT)}

\subsection{Overview}

To build a path-tree satisfying the specification of Equation \ref{eq:optimization_objective}, we proceed stepwise. First a transition system is grown in a sampling based fashion until the existence of a solution is guaranteed. This corresponds to the two first steps schematized on Figure \ref{algorithm_overview}. In a second step, the optimal path-tree is extracted using dynamic programming. 

\begin{figure}[!htb]
\scriptsize
\begin{tikzpicture}[>=latex] 
\node[myboxrectangle] (RRG) {Sampling of\\ Random Graph};
\node[myboxrectangle] (BS) [right =of RRG] {Graph expansion \\to Belief Space} edge [<-] (RRG);
\node[myboxrectangle] (PO) [right =of BS] {Path-tree\\Extraction} edge [<-] (BS);
\end{tikzpicture}
 \caption{ROPT Algorithm overview. It contains three main steps corresponding to the algorithms described in the sections \ref{section:rapidly-exploring-random-graph}, \ref{section:rapidly-exploring-random-graph} and \ref{section:policy-extraction}.}
 \label{algorithm_overview}
\end{figure}

\subsection{Interface between the algorithm and the application layer}

The connection between the core of the algorithm and a given planning problem is achieved via 4 functions that the application layer provides: 
\begin{itemize}
\item \textsc{StateCheck}, which takes a robot configuration $x$ as input and returns the list of worlds in which the configuration is valid.
\item \textsc{TransitionCheck}, which takes two robot configurations as inputs and returns the list of worlds in which the robot configurations is valid.
\item \textsc{GoalCheck}, which takes a robot configuration as input and returns the list of worlds in which the robot configuration fulfills the goal conditions.
\item \textsc{Observe}, which receives a robot configuration and a belief-state as inputs and returns the possible output belief states. %TODO: explain more% 
\end{itemize}

The functions \textsc{StateCheck}, \textsc{TransitionCheck} and \textsc{GoalCheck} are akin to the functions required by the RRT and PRM algorithms, but they are more general: the return type is a list of worlds instead of a boolean.

The \textsc{Observe} function is directly linked to partial observability and is specific to this algorithm. It allows belief-state inference. Going back to the example of Figure \ref{fig:problem}, \textsc{Observe} returns an unchanged belief-state for all robot configurations that are outside of the door visibility zone, this is because no observation can be received, and therefore no belief state inference is done. On the other hand, up to two possible belief-states are returned for robot configurations inside the door visibility zone. These two belief states correspond to the updated belief-states for the two possible observations.
%It must be noted that the observations are not explicitely handled by the algorithm

The following sections detail how the calls to those functions are orchestrated by the algorithm to build a path-tree.

\subsection{Rapidly-exploring Random Graph}
\label{section:rapidly-exploring-random-graph}

In this first step, a random-graph is grown in a sampling based fashion. To avoid the curse of dimensionality, sampling is not performed in belief-space directly but in the robot configuration space. The random graph is an intermediate representation and will be expanded to belief-state in a second step as described in the next section.

The nodes of the random graph are associated with:
\begin{itemize}
\item a robot configuration $x$, which is randomly sampled.
\item a list worlds $\mathcal{F}$ in which the robot configuration is fulfilling the goal condition. This is obtained by querying the function \textsc{GoalCheck}. 
\end{itemize} 

The edges of the random graph are associated with:
\begin{itemize}
\item a list of worlds $\mathcal{W}$ in which the transition is valid. This is obtained by calling the function \textsc{TransitionCheck}.
\end{itemize}

%robot configurations are sampled (state $x$), then the environment model is queried to know in which worlds this state is valid. For example. The nodes of the random graph are 

% Sampling takes place in the state-space: the continuous robot configuration $x$ and the discrete state $s$ are sampled. Although planning will ultimately be performed in belief-space, the sampling at this stage is done in the state-space to avoid the curse of dimensionality. The random graph will be expanded to belief-state in a second step (see Section \ref{section:graph_expansion_belief_space}).

The random-graph creation is described in Algorithm \ref{alg:rrg}. First, the state is sampled (lines 4). Then the new state is steered from a neighbor node of the graph (line 7). Unlike RRT where the new state is state is steered from the nearest neighbor, here, the selected neighbor is the nearest neighbor having world validities $\mathcal{W}$ containing the sampled world $w$ (line 5), as illustrated on Figure \ref{fig:expansion}. This additional condition for the neighbor selection is to ensure that the random graph contains paths to the goal for each world.
% similar to RRT / RRG quote RRG. The graph creation is done 
 
%The challenge is to efficiently grow a graph until completeness, i.e.\ up to a point guaranteeing that the next steps of the algorithm will be sucessfull (belief graph creation, path-tree extraction).

%unlike related work sample can be valid for certain worlds but not for others
%build transition system
%connect denser collision?free edges from this newly added vertex to its neighboring vertices within a defined radius ? .
% an off-line sampling-based algorithm for the construction of a global transition system

\begin{figure}[!htb]
 \center{\includegraphics[width=0.3\textwidth]{drawings/expansion.pdf}}
 \caption{Random Graph expansion: For a new sample \textit{a)}, the new state is computed steering from the closest compatible node of the graph. The closest compatible node depends on the sample world \textit{w}, see \textit{a)} and \textit{c}).}
 \label{fig:expansion}
\end{figure}
If the new state is valid for at least one world (line 9), the goal conditions are checked (line 10) and a new node is added to the random-graph (line 11).

Finally, the new node is connected to neighboring nodes (lines 13 to 16). The neighboring nodes are nodes within a given radius and which have world validities containing $w$.

This procedure is repeated until the graph is complete meaning that it will allow the successful extraction of a solution path-tree (see line 3). To implement the function \textsc{IsComplete} we assume in our examples that the transitions are \text-it{symmetrical}, i.e. if a motion exists from a node $u$ to $v$, then a motion between $v$ and $u$ also exists. Under this assumption, the random-graph contains a solution path-tree as soon as the set of leaf nodes is complete, in the sense that it covers every possible world, i.e.  $\bigcup_{u \in \mathcal{\mathcal{L}(G)}} \mathcal{F}_u = \mathcal{H}$. Indeed, the fact that the set of leaf nodes is complete implies that for each possible world $\forall s \in \mathcal{H}$, a path from the root to a leaf exists. An example of solution is therefore to execute those paths in sequence, and  potentially \textit{backtrack} to the root node if a path doesn't reach the goal (hence the required assumption regarding the symmetrical transition). In practice, once the completeness threshold is reached, a solution can be extracted which is typically way more optimal than the aforementioned worst-case backtracking strategy. In practice we also typically introduce a minimum number of iterations, to make sure that the random-graph is expanded even beyond the completeness threshold to improve the quality of the path-trees. The assumption of symmetric transitions is fair for the examples considered in the experimental section, it could however be restrictive in the case of kinodynamic planning. %we inten in future work to... 

\begin{algorithm}[H]
\caption{Rapidly-exploring Random Graph}
\label{alg:rrg}
\begin{algorithmic}[1]
\Function{BuildRRG}{$q_{start}$}
	\State $\mathcal{G}$.\Call{init}{$q_{start}$}
	\While{$\neg$ \Call{IsComplete}{$\mathcal{G}$}  }
    	\State $q_{rand} \gets$ \Call{SampleState()}{}
    	\State $w \gets$ \Call{SampleWorld()}{}
    	\State $q_{near} \gets$ \Call{Nearest}{$q_{rand}, w$}
    	\State $q_{new} \gets$ \Call{Steer}{$q_{near}, q_{rand}$}
    	\State \textcolor{cyan}{\footnotesize/*get relevant info of the new state and add it to the graph*/}
    	\If{\Call{StateCheck}{$q_{new}$} $\neq \emptyset$}
    	    \State $\mathcal{F} \gets$ \Call{GoalCheck}{$q_{new}$}
    		\State $\mathcal{G}$.\Call{AddNode}{$q_{new}, \mathcal{F}$}
    		\State \textcolor{cyan}{\footnotesize/*get relevant edges info and add them to the graph*/}
    		\For{$q_{near} \in$ \Call{Nearests}{$q_{new}, w$}}
    			\State $\mathcal{W} \gets$ \Call{TransitionCheck}{$q_{near}, q_{new}$}
    			\If{$\mathcal{W} \neq \emptyset$}
    				\State $\mathcal{G}$.\Call{AddEdge}{$q_{near}, q_{new}, \mathcal{W}$}
    			\EndIf
    		\EndFor
    	\EndIf
    \EndWhile
\EndFunction
\Statex
\end{algorithmic}
\end{algorithm}

\subsection{Graph expansion to belief space}
\label{section:graph_expansion_belief_space}
%doesn't contain any geometric checks anymore, or calls to nearest-neighbors
%figure with several layers
At this stage, a random-graph $\mathcal{G}$ has been built, and the existence of a path-tree solution is guaranteed, however the random-graph is still intermediate representation that can't be used directly to optimize path-trees. In this step, a transition system in belief-space, or belief-graph $\mathcal{B}$ is constructed out of $\mathcal{G}$ and by querying the observation model. 
The nodes of the belief-graph are associated with:
\begin{itemize}
\item a robot-configuration $x$.
\item a belief-state $b$.
\end{itemize}
The edges of the belief-graph are associated with:
\begin{itemize}
\item the observation $o$ making the transition between the beliefs on the incoming and out-coming nodes.
\end{itemize}
The belief-graph can be understood as the random-graph replicated over several layers, where each layer is a different belief-state, as shown on Figure \ref{fig:belief_layers}. The transitions within on layer correspond to robot motions, whereas the transitions from one layer to another correspond to the integration of an observation leading to a belief-state update. The motion transitions are, in general, not identical across belief-states. For example, on Figure \ref{fig:belief_layers}, the transitions crossing through the doors exist only in beliefs compatible with an open door.
  
\begin{figure}[!htb]
 \center{\includegraphics[width=0.45\textwidth]{drawings/belief_graph.pdf}}
 \caption{Random Graph expanded to belief state: The belief states are represented as \textit{layers} of the belief graph. The nodes within the observation range potentially lead to a belief state update and therefore have edges transitioning to other belief states (thick vertical edges).}
 \label{fig:belief_layers}
\end{figure}

The construction procedure is given by the Algorithm \ref{alg:belief}. First, the edges of $\mathcal{G}$ are replicated to the belief-states compatible with the edge's valid worlds (line 3 to 7).

Second, the observation model is called via the function \textsc{OBSERVE} to identify where edges should be created between belief-states (lines 9 to 12). The belief-state-transition edges due to observations are represented by vertical lines going from one layer to another on Figure \ref{fig:belief_layers}.

\begin{algorithm}[H]
\caption{Creation of the belief graph}
\label{alg:belief}
\begin{algorithmic}[1]
\Function{BuildBeliefGraph}{$b_{start}, \mathcal{G}$}
	\State \textcolor{cyan}{\footnotesize/*connect nodes within the same belief*/}
	\For{$e \in$ $\mathcal{G}.edges$}
		\For{$b \in$ \Call{Beliefs}{$e.world\_validities$}}
			\State $u \gets \mathcal{B}$.\Call{AddNode}{$b, e.from$}
			\State $v \gets \mathcal{B}$.\Call{AddNode}{$b, e.to$}
			\State $\mathcal{B}$.\Call{AddEdge}{$u, v$}
		\EndFor
	\EndFor
	\State \textcolor{cyan}{\footnotesize/*create transitions between beliefs due to observations*/}
	\For{$u \in$ $\mathcal{B}.nodes$}
	   	\State $V \gets$ \Call{Observe}{$u$}
		\For {$v \in V$}
			\State $\mathcal{B}$.\Call{AddEdge}{$u$, $v$}
		\EndFor
	\EndFor
\EndFunction
\end{algorithmic}
\end{algorithm}

%No need to query the collision checks anymore%
The belief-graph $\mathcal{B}$ is significantly larger than the random graph $\mathcal{G}$ due to the different belief-states corresponding to one same robot configuration. However, its construction only calls the observation model (function \textsc{Observe}). In contrast, the random-graph is smaller but its creation requires calls the collision checks (\textsc{StateCheck} and \textsc{TransitionCheck}) as well as the nearest neighbor search that are expensive.

\subsection{Policy extraction}
\label{section:policy-extraction}

At this stage, the belief-graph $\mathcal{B}$ is fully built, and it contains at least one path-tree satisfying the goal conditions (see \ref{eq:goal_constraint}). The goal is now to find the optimal path-tree by minimizing the expected costs to goal (see \ref{eq:cost_min}).

The expected costs to goal are computed for each node of $\mathcal{B}$. This is done using dynamic programming and by iteratively applying a Bellman updates on each node of $\mathcal{B}$.
The procedure is described by the Algorithm \ref{alg:expected_costs}.

The algorithm is similar to the Dijkstra algorithm \cite{Sniedovich2006DijkstrasAR} in the way the nodes are prioritized using a priority queue (lines 2 to 13). For edges corresponding to a robot motion, the Bellman update is also the same as in Dijkstra (lines 15 and 16). However, for edges corresponding to an observation, the Bellman update of the parent node differs and is the sum of the children expected costs weighted by their respective branching probabilities (lines 17 to 19).
\begin{algorithm}[H]
\caption{Computation of the expected costs to goal}
\label{alg:expected_costs}
\begin{algorithmic}[1]
\Function{ComputeExpectedCostToGoal}{$\mathcal{B}$}
	\State $Q \gets $ \Call{PriorityQueue()}{}
	\State \textcolor{cyan}{\footnotesize/*Initialization*/}
	\For{$n \in$ $\mathcal{B}.nodes$}
		\If{\Call{IsFinal}{$n$}}
			\State $C[n] \gets 0.0$
			\State $Q$.\Call{Push}{$n, 0.0$}
		\Else
			\State $C[n] \gets +\infty$
		\EndIf
	\EndFor
	\State \textcolor{cyan}{\footnotesize/*The main loop*/}
	\While{$\neg$ \Call{IsEmpty}{$Q$}}
		\State $v \gets$ \Call{Pop}{$Q$}
		\For {$u \in$ \Call{Parents}{$v$}}
			\State \textcolor{cyan}{\footnotesize/*Bellman update dependent on the edge type*/}
			\If{\Call{IsActionEdge}{$u, v$}}
				\State $c \gets$ \Call{Cost}{$u$}$+ C[v]$
			\ElsIf{\Call{IsObservationEdge}{$u, v$}}
				\State $\mathcal{W} \gets $ \Call{ObservationChildren}{$u$}
				\State $c \gets \sum_{\nu \in \mathcal{W}}{ p(\nu | u) \times C[w]} $
			\EndIf
			\If{$c < C[u]$}
				\State $C[u] \gets c$
				\State $Q$.\Call{Push}{$u, c$}
			\EndIf
		\EndFor
	\EndWhile
\EndFunction
\end{algorithmic}
\end{algorithm}

Once the expected costs to goal are known for each node, the optimal path-tree can be built straightforwardly starting from the root and recursively appending the best next child, or next best children (in case of a node with an observation branching).

Optimizing w.r.t. the expected costs to goal naturally results in an exploration vs. exploitation trade-off, i.e. path-trees balance the need for moving towards configurations  providing informative observations vs. the need to advance towards the goal.
%balance exploration and exploitation

\subsection{Path-tree refinement}
%partial shortcut on trajectory pieces%
At this stage, a solution path-tree has already been extracted. However path-trees may contain  unnatural or jerky motions due to the random nature of the random-graph creation. One way to reduce this phenomenon is to have a high minimal number of iterations when creating the random-graph. However, this can become quite costly in terms of runtime. In practice, it is also very fast and efficient to improve the path-tree in a final refinement step. We refine the path-tree piecewise: the path-pieces between observation branchings are refined independently using the partial-shortcut method \cite{geraerts2007creating}. The branching nodes corresponding to observations are extremities of the path pieces and they form the junctions between path pieces. Those nodes are not modified by the refinement procedure. 

%Obtaining near-optimal observation points therfore relies on having  
%\subsection{Complexity analysis} -> keep for Phd
% rename to algoritmic properties (completness, optimality)

% (i.e., it finds a solution with probability 1 if one exists and the number of samples approaches infinity)

%w.r.t. the number of beliefs
% different problems lead to different relation n_world <-> n_beliefs

\subsection{Completeness and optimality}


\section{EXPERIMENTS}
The path-planner core algorithm is implemented in the Rust programming language \cite{matsakis2014rust}. The application layer is implemented in C++ using MoveIt \cite{coleman2014reducing}. The source code and a supplementary video are available for reference \footnotemark \footnotetext{\href{https://github.com/cambyse/po-rrt}{https://github.com/cambyse/po-rrt}}. %TODO put real url

\subsection{Mobile robot navigation} \label{experiment:navigation}

\subsection{Robot arm object fetching} \label{experiment:mobile_manipulation}
In these two examples, the robot is composed of a Panda arm mounted on a mobile base. The robot task is to pick-up an object which location is unknown (see Figure \ref{fig:arm_example_1} and \ref{fig:arm_example_2}). The robot has prior knowledge of several potential locations, but the actual location is unknown at planning time. 

%. In other words, the perception pipeline will detect the object if the object is within the filed of view, at a distance closer than 2 meters from the sensor, and that the object is not occluded (e.g. by the walls).

%TODO: 
% - put tables to avoid the unused slots?
% - just oneblock?
% - picture sensor field of view etc?
\begin{figure}[!htb]
 \center{\includegraphics[width=0.45\textwidth]{drawings/2_arm/example_1.png}}
 \caption{Example 1: The robot has to pick the green block. The block location is unknown, it maybe on each one of the three shelves.}
 \label{fig:arm_example_1}
\end{figure}

The observation model simulates that a sensor is placed on the robot gripper and that the perception pipeline detects the block when it is within the sensor field of view (60$^{\circ}$), at a distance less 2 meters that from the sensor, and not occluded by other objects (e.g. the walls).

\begin{figure}[!htb]
 \center{\includegraphics[width=0.45\textwidth]{drawings/2_arm/example_2.png}}
 \caption{Example 2: The block might be on two different shelves. The opening between the two doors potentially allows the robot to observe the shelves without entering the rooms.}
 \label{fig:arm_example_2}
\end{figure}

Planning is performed in joint space with 9 degrees of freedom (2 for the base, and 7 for the robot arm).

Figure \ref{fig:example_2_view_point} shows the trajectory of the robot base along the path tree. There are two observation branching points corresponding to the observations of the shelves 0 an 1.

\begin{figure}[!htb]
 \center{\includegraphics[width=0.45\textwidth]{drawings/2_arm/example_1_path_tree.png}}
 \caption{Path-tree: The planned path tree first moves towards the shelf\_0 to reach the first viewpoint. If the object is on the shelf\_0, the green path is executed. Otherwise the robot moves towards the second viewpoint. Based on the observation, the robot executes either the magenta or the orange path.}
 \label{fig:example_1_path_tree}
\end{figure}

\begin{figure}[!htb]
 \center{\includegraphics[width=0.45\textwidth]{drawings/2_arm/example_2_viewpoint.png}}
 \caption{Observation branching point: A first common branch of the path-tree leads the robot to an observation point through the opening. Based on the received observation, the robot will either execute the yellow or the magenta path.}
 \label{fig:example_2_view_point}
\end{figure}

%example of path tree, show the view-point

%table runtime, first 

%influence of the number of iterations on thequality

% influence of the belief state

% Observation model
% Runtime per algorithm stage

\begin{table}[h]
\begin{center}
\begin{tabular}{|c||c|c|c|c||c|}
\hline
  & \thead{\# of\\ iter} & \thead{random\\graph\\creation} & \thead{belief-\\space\\expansion} & \thead{policy\\ extraction} & \thead{planning\\time\\(ms)} \\
\hline
example-1  & & & & &\\
 & & & & &\\
\hline
example-2  & & & & &\\
 & & & & &\\
\hline
\end{tabular}
\end{center}
\caption{Planning times:.}
\label{tab:planning_times}
\end{table}

% variation 3 locations vs 9 locations or tables?

\subsection{Comparison to baseline} \label{experiment:baseline comparison}
We compare PTO to a Task and Motion Planning (TAMP) approach, where the planning is hierarchical: it interleaves a symbolic search defining high level plans, and a motion planning phase computing the corresponding motions. The comparison is performed on variations of the shelf domain (see Figure \ref{fig:baseline_comparison}), and where we plan for the  2D base position only. The number of shelves is varied to compare the scalability.

The high level search is performed using the Branch and Bound algorithm. It explores the space of the possible sequences for visiting all the shelves. The nodes of the search correspond to shelves. The motion planner is queried when a node is created. For each node/shelf, 2 paths are planned using RRT$^{*}$: the path to an observation point, and the path to pick-up the object (to be executed if the object is actually detected). A path-tree is obtained by gathering the path pieces from the root node to a leaf of the search tree. 

The branch and bound tree seach is performed in a Depth-First fashion, which leads to a quick first solution path tree. The best solution path-tree found so far is an upper bound of the cost and allows the pruning of a majority of the search tree.

%The high level planning is performed as a Branch and Bound tree search. where the nodes correspond to shelves. The corresponding motions are computed when exanding the tree. For each shelf, 2 paths are planned using RRT$^{*}$: the path to an observation point, and the path to pick-up the object (to be executed if the object is actually detected). A path-tree is obtained by gathering the path pieces planned from the root to a leaf node.

We report on Figure \ref{fig:costs_and_runtime} on the expected cost of the resulting path-trees, as well as on the overall planning time.

%The high-level plans are ordered sequences of shelves to inspect, they are obtained in a breadth-first fashion. For each shelf, 2 paths are planned using RRT$^{*}$: the path to the shelf itself (to pick the object if it is there), and the path towards the next observation point (if the object is not there). The path-tree is obtained by gathering the path pieces planned separately.%

%Branch and bound, depth first, upper bound
%no heuristic
%100 iterations
%advantges: optimality, faster execution times, lighweight setup. No need to bring domain knowledge to set up an action space for a high level search.


\begin{figure}[!htb]
 \center{\includegraphics[width=0.45\textwidth]{drawings/2_arm/baseline_comparison.png}}
 \caption{Comparison of PTO to RRT$^{*}$: The green path-tree (PTO) has lower expected cost than the orange path-tree planned piecewise using RRT$^{*}$.}
 \label{fig:baseline_comparison}
\end{figure}


\pgfplotsset{width=5cm, height=7cm}

\begin{figure}[h]  
\centering
  \begin{subfigure}[b]{0.45\linewidth}
    \begin{tikzpicture}
	 \begin{axis} [
	    ybar = .05cm,
    	bar width = 7pt,
    	ymin = 0,
    	ymax = 3,
    	xtick = data,
    	enlarge y limits = {value = .25, upper},
    	enlarge x limits = {abs = .8}
		]
		\addplot [green!20!black,fill=green!80!white] coordinates {
		(2, 1.7529471182990233) (4, 2.2212974330517827) (6, 2.3911496782081954) (8, 2.360321441477037)
		};
		\addplot [orange!20!black,fill=orange!80!white] coordinates {
		(2, 1.8569080235157531) (4, 2.3748431227440063) (6, 2.6112924196096783) (8, 2.5971396786963075)
		};
		\legend {PTO, RRT$^{*}$}; 
		\end{axis}
	\end{tikzpicture}
    \caption{Path costs (m)}
    \label{fig:y equals x}
  \end{subfigure}
  \begin{subfigure}[b]{0.45\linewidth}
    \begin{tikzpicture}
	 \begin{axis} [
	    ybar = .05cm,
	    ymin = 0.05,
    	bar width = 7pt,
    	ymode=log,
    	log origin=infty,
    	xtick = data,
    	enlarge y limits = {value = .25, upper},
    	enlarge x limits = {abs = .8},
    	legend pos=north west
		]
		\addplot [green!20!black,fill=green!80!white] coordinates {
		(2, 0.0838456953) (4, 0.3447605707) (6, 1.3742766512) (8, 7.585573182200001)
		};
		\addplot [orange!20!black,fill=orange!80!white] coordinates {
		(2, 0.0763110857) (4, 0.8065830762000001) (6, 11.0703240974) (8, 107.3652537214)
		};
		\legend {PTO, RRT$^{*}$}; 
		\end{axis}
	 \end{tikzpicture}
     \caption{Planning time (s)}
     \label{fig:y equals x}
   \end{subfigure}
   \caption{Comparison of path-tree costs and planning times. PTO provides path-trees with lower costs. Planning time also scales better w.r.t. the number of shelves. }
   \label{fig:costs_and_runtime}
\end{figure}

First, We observe that the PTO consistently leads to path-tree costs that are lower than the piece-wise planning with RRT$^{*}$. This can be understood easily, the decomposition into piecewise motions leads to path pieces that are optimal w.r.t the subproblem of reaching one shelf, but taken together, al lthe path-pieces don't result in an optimal path-tree. On the other hand, PTO searches for a globally optimal path-tree. It is visible on Figure \ref{fig:baseline_comparison}, the RRT$^{*}$ path tree greedily moves towards the next shelf to explore, whereas PTO tends to take a less direct path to the observation point, but which overall leads to a shorter path-tree. 

Second, PTO scales better w.r.t. the number of shelves (see \ref{fig:costs_and_runtime}. Although planning times are comparable for the variation with 2 shelves, the difference between PTO and RRT becomes larger and larger when the number of shelves increases. The main reason for this is that PTO is more sampling efficient: The random graph (see \ref{section:rapidly-exploring-random-graph}) which involves state and transition geometric validity checks is constructed just once. On the other hand, Branch and Bound + RRT$^{*}$ approach is more sampling intensive since a new random tree is resampled from scratch for the planning of each path-piece.

It is also worth noting that, in this particular example, PRM could be used to improve the runtime for the baseline. On the other hand, this would not be a general solution. Indeed, the state validity depends, in general, on the state $s$ (see \ref{experiment:navigation}), such that multiple roadmaps would be needed.

In addition, in this example, both algorithms PTO and the Branch and Bound search could be made faster by using heuristics as lower bound of the path costs (e.g. euclidean distance). We chose, however, to compare here the general approaches, without optimization tailored to the particular example.  
% Note about PRM
% simple example
% RRT TAMP Task and Motion Planning
% ni usage of heuristics
\section{CONCLUSIONS AND FUTURE WORK}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{ACKNOWLEDGMENT}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bibliography{references}
\bibliographystyle{ieeetr}

\end{document}
